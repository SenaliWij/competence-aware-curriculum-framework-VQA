# -*- coding: utf-8 -*-
"""checkpoint_service.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14femBfOcfsxWWEAds7r4WUXS3TPWbyet
"""

import torch
import os
import shutil
from typing import Dict, Any, Optional

class CheckpointManager:
    """
    Manages saving and loading of training checkpoints.
    Includes 'latest' resume capability and 'best' model tracking per tier.
    """
    def __init__(self, checkpoint_dir: str = "checkpoints"):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        self.best_metrics: Dict[int, float] = {} # tier -> best_acc
        self.best_overall_acc = 0.0 # Overall best accuracy

    def save(self,
             model_state: Dict,
             optimizer_state: Dict,
             curriculum_state: Dict,
             metrics: Dict,
             is_best: bool = False,
             tier: int = 1):
        """
        Saves a checkpoint.
        """
        state = {
            'model_state_dict': model_state,
            'optimizer_state_dict': optimizer_state,
            'curriculum_state': curriculum_state,
            'metrics': metrics,
            'tier': tier
        }

        # Save latest
        latest_path = os.path.join(self.checkpoint_dir, "checkpoint_latest.pt")
        torch.save(state, latest_path)

        # If best for this tier
        if is_best:
            best_path = os.path.join(self.checkpoint_dir, f"checkpoint_tier{tier}_best.pt")
            shutil.copyfile(latest_path, best_path)
            print(f"Saved best model for Tier {tier} to {best_path}")

        # Check overall best
        current_acc = metrics.get('accuracy', 0.0)
        if current_acc > self.best_overall_acc:
            self.best_overall_acc = current_acc

            # Update state with new best (need to re-save latest to persist this?
            # Or just save best separately. Ideally, 'latest' should know the new best_overall_acc)
            # Simplest way: just copy latest to best_overall
            best_overall_path = os.path.join(self.checkpoint_dir, "checkpoint_best_overall.pt")
            shutil.copyfile(latest_path, best_overall_path)
            print(f"*** NEW OVERALL BEST! Acc: {self.best_overall_acc:.4f} -> Saved to {best_overall_path}")

            # Update the latest checkpoint to include this new best record so if we crash we know it
            state['best_overall_acc'] = self.best_overall_acc
            torch.save(state, latest_path) # Overwrite latest with updated metadata



    def load_latest(self, model, optimizer=None, curriculum=None):
        """
        Loads the latest checkpoint if it exists.
        Returns: start_tier, metrics (or defaults if no checkpoint)
        """
        latest_path = os.path.join(self.checkpoint_dir, "checkpoint_latest.pt")
        if not os.path.exists(latest_path):
            print("No checkpoint found. Starting from scratch.")
            return 1, None

        print(f"Loading checkpoint from {latest_path}...")
        checkpoint = torch.load(latest_path)

        model.load_state_dict(checkpoint['model_state_dict'])
        if optimizer and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        if curriculum and 'curriculum_state' in checkpoint:
            curriculum.load_config_state(checkpoint['curriculum_state'])

        tier = checkpoint.get('tier', 1)
        metrics = checkpoint.get('metrics', None)
        self.best_overall_acc = checkpoint.get('best_overall_acc', 0.0)
        print(f"Resumed at Tier {tier}. Best Overall Acc: {self.best_overall_acc:.4f}")
        return tier, metrics

