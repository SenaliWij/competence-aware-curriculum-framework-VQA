# -*- coding: utf-8 -*-
"""clevr_dataset.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vzjh5HmGwROdEV3vjGXUNHzo2GySCP07
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Core dependencies
# !pip install -q transformers torch torchvision pillow
# 
# # Optional but recommended
# !pip install -q tqdm  # For progress bars during training
# !pip install -q wandb  # For experiment tracking (optional)

# from google.colab import drive
# drive.mount('/content/drive')

"""### **1. ENCODING IMAGES AND QUESTIONS USING ViLT PROCESSOR**"""

import io
import json
from typing import Dict, List, Optional, Any

import boto3
import torch
from torch.utils.data import Dataset
from PIL import Image
from transformers import ViltProcessor



"""Utils functions

"""

class S3Client:
    def __init__(self, bucket: str):
        self.bucket = bucket
        self.client = boto3.client("s3")

    def load_json(self, key: str) -> Any:
        obj = self.client.get_object(Bucket=self.bucket, Key=key)
        return json.loads(obj["Body"].read().decode("utf-8"))

    def load_image(self, key: str) -> Image.Image:
        obj = self.client.get_object(Bucket=self.bucket, Key=key)
        return Image.open(io.BytesIO(obj["Body"].read())).convert("RGB")

def build_answer_vocab_s3(
    s3: S3Client,
    question_keys: List[str]
) -> Dict[str, int]:
    answers = set()
    for key in question_keys:
        data = s3.load_json(key)["questions"]
        for q in data:
            if "answer" in q:
                answers.add(str(q["answer"]).strip().lower())
    answers = sorted(answers)
    return {a: i for i, a in enumerate(answers)}


"""Dataset class (loads image + question and encodes using ViLT)"""
class CLEVRCurriculumViltDatasetS3(Dataset):
    def __init__(
        self,
        bucket: str,
        images_prefix: str,        # "image"
        questions_prefix: str,     # "questions"
        processor: ViltProcessor,
        split: str,
        answer2id: Optional[Dict[str, int]] = None,
        tiers: Optional[List[int]] = None,
        max_length: int = 32,
    ):
        assert split in {"train", "val", "test"}

        self.s3 = S3Client(bucket)
        self.images_prefix = images_prefix
        self.questions_prefix = questions_prefix
        self.processor = processor
        self.split = split
        self.answer2id = answer2id
        self.max_length = max_length

        self.samples: List[Dict[str, Any]] = []

        # ---------- LOAD QUESTIONS (FROM S3) ----------
        if split in {"train", "val"} and tiers is not None:
            for t in tiers:
                qkey = f"{questions_prefix}/CLEVR_{split}_questions_L{t}.json"
                questions = self.s3.load_json(qkey)["questions"]

                for q in questions:
                    self.samples.append({
                        "question": q["question"],
                        "answer": q.get("answer"),
                        "image_filename": q["image_filename"],
                        "question_index": q.get("question_index", -1),
                        "tier": t,
                    })
        else:
            qkey = f"{questions_prefix}/CLEVR_{split}_questions.json"
            questions = self.s3.load_json(qkey)["questions"]

            for q in questions:
                self.samples.append({
                    "question": q["question"],
                    "answer": q.get("answer"),
                    "image_filename": q["image_filename"],
                    "question_index": q.get("question_index", -1),
                    "tier": -1,
                })


    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        s = self.samples[idx]

        image_key = (
            f"{self.images_prefix}/"
            f"{self.split if self.split != 'val' else 'val'}/"
            f"{s['image_filename']}"
        )

        image = self.s3.load_image(image_key)

        enc = self.processor(
            images=image,
            text=s["question"],
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
        )

        item = {k: v.squeeze(0) for k, v in enc.items()}

        if s.get("answer") is not None and self.answer2id is not None:
            key = str(s["answer"]).strip().lower()
            item["labels"] = torch.tensor(self.answer2id[key], dtype=torch.long)

        item["tier"] = torch.tensor(s["tier"], dtype=torch.long)
        item["question_id"] = torch.tensor(s["question_index"], dtype=torch.long)
        return item



def vilt_collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
    out: Dict[str, torch.Tensor] = {}
    for k in batch[0].keys():
        if k == "labels":
            continue
        out[k] = torch.stack([b[k] for b in batch])
    if all("labels" in b for b in batch):
        out["labels"] = torch.stack([b["labels"] for b in batch])
    return out

"""Encoding process(Build answer vocab + Create dataset + dataloader +  Save encoded batch (.pt))"""

# # Configuration (paths + processor)
# QUESTIONS_DIR = "/content/drive/MyDrive/Colab Notebooks/FYP/dataset/clevr_kaggle/CLEVR_v1.0/questions"
# IMAGES_DIR    = "/content/drive/MyDrive/Colab Notebooks/FYP/dataset/clevr_kaggle/CLEVR_v1.0/images"

# processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")

# # Build answer vocab (from all train tiers)
# tier_paths = [os.path.join(QUESTIONS_DIR, f"CLEVR_train_questions_L{i}.json") for i in [1,2,3,4,5]]
# answer2id = build_answer_vocab(tier_paths)
# print("Answer vocab size:", len(answer2id))

# # Create dataset + dataloader (Tier 1 only)

# train_ds_L1 = CLEVRCurriculumViltDataset(
#     questions_dir=QUESTIONS_DIR,
#     images_dir=IMAGES_DIR,
#     processor=processor,
#     split="train",
#     tiers=[1],
#     answer2id=answer2id,
#     max_length=32,
# )

# train_loader_L1 = DataLoader(
#     train_ds_L1,
#     batch_size=16,
#     shuffle=True,
#     num_workers=0,  # keep 0 while using Google Drive
#     collate_fn=vilt_collate_fn,
# )


# # Sanity check (get one batch + print shapes)

# batch = next(iter(train_loader_L1))
# for k, v in batch.items():
#     print(k, v.shape, v.dtype)

# # Save encoded batch (.pt)

# OUT_DIR = "/content/drive/MyDrive/Colab Notebooks/FYP/encode_output"
# os.makedirs(OUT_DIR, exist_ok=True)

# batch_path = os.path.join(OUT_DIR, "sanity_batch_L1.pt")
# torch.save(batch, batch_path)
# print("Saved:", batch_path)

