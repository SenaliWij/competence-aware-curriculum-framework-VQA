{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SenaliWij/competence-aware-curriculum-framework-VQA/blob/master/clevr_dataset_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5OVntEM48vC",
        "outputId": "b8ef9481-e270-4897-da53-7673ae91cc38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.95 s, sys: 424 ms, total: 3.37 s\n",
            "Wall time: 10.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Core dependencies\n",
        "!pip install -q transformers torch torchvision pillow\n",
        "\n",
        "# Optional but recommended\n",
        "!pip install -q tqdm  # For progress bars during training\n",
        "!pip install -q wandb  # For experiment tracking (optional)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpq2KQ5RNrJO",
        "outputId": "164e6872-dab4-4603-db60-99e6f5e7d5b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. ENCODING IMAGES AND QUESTIONS USING ViLT PROCESSOR**"
      ],
      "metadata": {
        "id": "sO9UzMyR6zXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from transformers import ViltProcessor"
      ],
      "metadata": {
        "id": "aI-7eWDEbRnG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils functions\n"
      ],
      "metadata": {
        "id": "2352AsjvbVEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_questions(path: str) -> List[Dict[str, Any]]:\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)[\"questions\"]\n",
        "\n",
        "\n",
        "def build_answer_vocab(paths: List[str]) -> Dict[str, int]:\n",
        "    answers = set()\n",
        "    for p in paths:\n",
        "        data = load_questions(p)\n",
        "        for q in data:\n",
        "            if \"answer\" in q:\n",
        "                answers.add(str(q[\"answer\"]).strip().lower())\n",
        "    answers = sorted(answers)\n",
        "    return {a: i for i, a in enumerate(answers)}"
      ],
      "metadata": {
        "id": "rKFEK5HDbT2t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset class (loads image + question and encodes using ViLT)"
      ],
      "metadata": {
        "id": "J7h-lyrmcbUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLEVRCurriculumViltDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        questions_dir: str,\n",
        "        images_dir: str,\n",
        "        processor: ViltProcessor,\n",
        "        split: str,\n",
        "        answer2id: Optional[Dict[str, int]] = None,\n",
        "        tiers: Optional[List[int]] = None,     # only used for train\n",
        "        max_length: int = 32,\n",
        "    ):\n",
        "        assert split in {\"train\", \"val\", \"test\"}\n",
        "        self.questions_dir = questions_dir\n",
        "        self.images_dir = images_dir\n",
        "        self.processor = processor\n",
        "        self.split = split\n",
        "        self.answer2id = answer2id\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # sanity: ensure split image folder exists\n",
        "        self.split_img_dir = os.path.join(images_dir, split if split != \"val\" else \"val\")\n",
        "        if not os.path.isdir(self.split_img_dir):\n",
        "            raise FileNotFoundError(f\"Image folder not found in Colab FS: {self.split_img_dir}\")\n",
        "\n",
        "        self.samples: List[Dict[str, Any]] = []\n",
        "\n",
        "        use_tiers = (split in {\"train\", \"val\"}) and (tiers is not None)\n",
        "\n",
        "        if use_tiers:\n",
        "            # For tiered val/train, default tiers if not provided (optional)\n",
        "            if tiers is None:\n",
        "                tiers = [1, 2, 3, 4, 5]\n",
        "\n",
        "            for t in tiers:\n",
        "                qpath = os.path.join(questions_dir, f\"CLEVR_{split}_questions_L{t}.json\")\n",
        "                if not os.path.exists(qpath):\n",
        "                    raise FileNotFoundError(f\"Tier file not found: {qpath}\")\n",
        "\n",
        "                for q in load_questions(qpath):\n",
        "                    self.samples.append(\n",
        "                        {\n",
        "                            \"question\": q[\"question\"],\n",
        "                            \"answer\": q.get(\"answer\"),  # train/val should have answers\n",
        "                            \"image_filename\": q[\"image_filename\"],\n",
        "                            \"question_index\": q.get(\"question_index\", -1),\n",
        "                            \"tier\": t,\n",
        "                        }\n",
        "                    )\n",
        "        else:\n",
        "            qfile = f\"CLEVR_{split}_questions.json\"\n",
        "            qpath = os.path.join(questions_dir, qfile)\n",
        "            if not os.path.exists(qpath):\n",
        "                raise FileNotFoundError(f\"{split} questions file not found: {qpath}\")\n",
        "\n",
        "            for q in load_questions(qpath):\n",
        "                self.samples.append(\n",
        "                    {\n",
        "                        \"question\": q[\"question\"],\n",
        "                        \"answer\": q.get(\"answer\"),  # val has answers; test may not\n",
        "                        \"image_filename\": q[\"image_filename\"],\n",
        "                        \"question_index\": q.get(\"question_index\", -1),\n",
        "                        \"tier\": -1,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        s = self.samples[idx]\n",
        "        img_path = os.path.join(self.split_img_dir, s[\"image_filename\"])\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        enc = self.processor(\n",
        "            images=image,\n",
        "            text=s[\"question\"],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "\n",
        "        # labels if available\n",
        "        if s.get(\"answer\") is not None and self.answer2id is not None:\n",
        "            key = str(s[\"answer\"]).strip().lower()\n",
        "            item[\"labels\"] = torch.tensor(self.answer2id[key], dtype=torch.long)\n",
        "\n",
        "        item[\"tier\"] = torch.tensor(s[\"tier\"], dtype=torch.long)\n",
        "        item[\"question_id\"] = torch.tensor(s[\"question_index\"], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "\n",
        "def vilt_collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "    out: Dict[str, torch.Tensor] = {}\n",
        "    for k in batch[0].keys():\n",
        "        if k == \"labels\":\n",
        "            continue\n",
        "        out[k] = torch.stack([b[k] for b in batch])\n",
        "    if all(\"labels\" in b for b in batch):\n",
        "        out[\"labels\"] = torch.stack([b[\"labels\"] for b in batch])\n",
        "    return out"
      ],
      "metadata": {
        "id": "xHBylXMwXfoy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding process(Build answer vocab + Create dataset + dataloader +  Save encoded batch (.pt))"
      ],
      "metadata": {
        "id": "P3qOXss6cGd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Configuration (paths + processor)\n",
        "# QUESTIONS_DIR = \"/content/drive/MyDrive/Colab Notebooks/FYP/dataset/clevr_kaggle/CLEVR_v1.0/questions\"\n",
        "# IMAGES_DIR    = \"/content/drive/MyDrive/Colab Notebooks/FYP/dataset/clevr_kaggle/CLEVR_v1.0/images\"\n",
        "\n",
        "# processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
        "\n",
        "# # Build answer vocab (from all train tiers)\n",
        "# tier_paths = [os.path.join(QUESTIONS_DIR, f\"CLEVR_train_questions_L{i}.json\") for i in [1,2,3,4,5]]\n",
        "# answer2id = build_answer_vocab(tier_paths)\n",
        "# print(\"Answer vocab size:\", len(answer2id))\n",
        "\n",
        "# # Create dataset + dataloader (Tier 1 only)\n",
        "\n",
        "# train_ds_L1 = CLEVRCurriculumViltDataset(\n",
        "#     questions_dir=QUESTIONS_DIR,\n",
        "#     images_dir=IMAGES_DIR,\n",
        "#     processor=processor,\n",
        "#     split=\"train\",\n",
        "#     tiers=[1],\n",
        "#     answer2id=answer2id,\n",
        "#     max_length=32,\n",
        "# )\n",
        "\n",
        "# train_loader_L1 = DataLoader(\n",
        "#     train_ds_L1,\n",
        "#     batch_size=16,\n",
        "#     shuffle=True,\n",
        "#     num_workers=0,  # keep 0 while using Google Drive\n",
        "#     collate_fn=vilt_collate_fn,\n",
        "# )\n",
        "\n",
        "\n",
        "# # Sanity check (get one batch + print shapes)\n",
        "\n",
        "# batch = next(iter(train_loader_L1))\n",
        "# for k, v in batch.items():\n",
        "#     print(k, v.shape, v.dtype)\n",
        "\n",
        "# # Save encoded batch (.pt)\n",
        "\n",
        "# OUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/FYP/encode_output\"\n",
        "# os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# batch_path = os.path.join(OUT_DIR, \"sanity_batch_L1.pt\")\n",
        "# torch.save(batch, batch_path)\n",
        "# print(\"Saved:\", batch_path)\n"
      ],
      "metadata": {
        "id": "nH2_9MtWaE3w"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y5tJdxEOdIfD"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}