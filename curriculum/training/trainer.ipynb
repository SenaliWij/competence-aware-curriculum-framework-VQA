{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "S3_BUCKET = \"s3://clevr-dataset\"\n",
        "LOCAL_DATA_DIR = \"./dataset\"\n",
        "\n",
        "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
        "\n",
        "!aws s3 sync {S3_BUCKET} {LOCAL_DATA_DIR}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdJGfvp_CvOC",
        "outputId": "a4170a64-5ad9-46ed-e88d-e419a65ff4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "jgx7TtS_9O_M",
        "outputId": "37242f92-5bae-43f9-c83a-880a3a06dcbd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'evaluation_service'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4218998637.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mevaluation_service\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluationService\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcurriculum_service\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCurriculumManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcheckpoint_service\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCheckpointManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluation_service'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "import tqdm\n",
        "from typing import Optional\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import ViltProcessor\n",
        "\n",
        "# Core\n",
        "from evaluation_service import EvaluationService\n",
        "from curriculum_service import CurriculumManager\n",
        "from checkpoint_service import CheckpointManager\n",
        "\n",
        "# Model\n",
        "from vilt_adapter import ViLTAdapter\n",
        "\n",
        "# Data\n",
        "from clevr_dataset_py import CLEVRCurriculumViltDataset,vilt_collate_fn,build_answer_vocab\n",
        "CHECKPOINT_ROOT = \"/content/drive/MyDrive/Colab Notebooks/FYP/checkpoints\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VJMuRdgk-rtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CurriculumTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        questions_dir: str,\n",
        "        images_dir: str,\n",
        "        answer2id: dict,\n",
        "        run_name: str = \"curriculum_run_v1\",\n",
        "        output_dir: str = \"./outputs\",\n",
        "        batch_size: int = 32,\n",
        "        max_tiers: int = 5,\n",
        "        use_sspl: bool = False\n",
        "    ):\n",
        "        self.output_dir = os.path.join(output_dir, run_name)\n",
        "        self.questions_dir = questions_dir\n",
        "        self.images_dir = images_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.answer2id = answer2id\n",
        "\n",
        "        # Components\n",
        "        self.evaluation_service = EvaluationService()\n",
        "        self.curriculum = CurriculumManager(max_tiers=max_tiers)\n",
        "        self.checkpoint_manager = CheckpointManager(checkpoint_dir=os.path.join(CHECKPOINT_ROOT, run_name))\n",
        "\n",
        "        # Model\n",
        "        self.model_adapter = ViLTAdapter() # This loads the model and optimizer\n",
        "\n",
        "        # Processor (needed for dataset)\n",
        "        self.processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
        "\n",
        "        self.use_sspl = use_sspl\n",
        "\n",
        "    def data_loader_for_tier(\n",
        "        self,\n",
        "        tier: int,\n",
        "        split: str = \"train\",\n",
        "        shuffle: bool = True,\n",
        "    ) -> DataLoader:\n",
        "        \"\"\"\n",
        "        Creates a DataLoader for a specific tier using user's custom logic structure.\n",
        "        \"\"\"\n",
        "        tiers = [tier] if tier is not None else None\n",
        "\n",
        "        # Determine dataset parameters based on split\n",
        "        # Note: The user's snippet hardcoded split logic inside the call, here we make it dynamic\n",
        "\n",
        "        dataset_sample = CLEVRCurriculumViltDataset(\n",
        "            questions_dir=self.questions_dir,\n",
        "            images_dir=self.images_dir,\n",
        "            processor=self.processor,\n",
        "            split=split,\n",
        "            tiers=tiers,\n",
        "            answer2id=self.answer2id if split in [\"train\", \"val\"] else None, # answer2id mostly needed for train/val\n",
        "            max_length=32, # Default max length\n",
        "        )\n",
        "\n",
        "        loader = DataLoader(\n",
        "            dataset_sample,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=shuffle if split == \"train\" else False,\n",
        "            num_workers=0, # As requested for Drive/Colab compatibility\n",
        "            pin_memory=True,\n",
        "            collate_fn=vilt_collate_fn,\n",
        "        )\n",
        "        return loader\n",
        "\n",
        "def train(self):\n",
        "        \"\"\"\n",
        "        Main training loop.\n",
        "        Adapted for dynamic epochs and patience.\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Load State\n",
        "        start_tier, stored_metrics = self.checkpoint_manager.load_latest(\n",
        "            self.model_adapter.model,\n",
        "            self.model_adapter.optimizer,\n",
        "            self.curriculum\n",
        "        )\n",
        "\n",
        "        print(f\"Starting/Resuming at Tier {start_tier}\")\n",
        "\n",
        "        # 2. Continuous Loop until Curriculum Completed\n",
        "        while not self.curriculum.is_completed:\n",
        "            current_tier = self.curriculum.current_tier\n",
        "            print(f\"\\n{'='*20}\\n Entering Tier {current_tier} \\n{'='*20}\")\n",
        "\n",
        "            # Setup Data (Re-initialize loaders for the current tier)\n",
        "            train_loader = self.data_loader_for_tier(\n",
        "                tier=current_tier,\n",
        "                split='train',\n",
        "                shuffle=True\n",
        "            )\n",
        "\n",
        "            val_loader = self.data_loader_for_tier(\n",
        "                tier=current_tier,\n",
        "                split='val',\n",
        "                shuffle=False\n",
        "            )\n",
        "\n",
        "            # Patience Configuration\n",
        "            patience_limit = 3\n",
        "            patience_counter = 0\n",
        "            best_acc_in_tier = 0.0\n",
        "            epoch_in_tier = 0\n",
        "\n",
        "            # Infinite epoch loop for this tier (broken by advancement or patience)\n",
        "            while self.curriculum.current_tier == current_tier:\n",
        "                epoch_in_tier += 1\n",
        "                print(f\"\\nTier {current_tier} - Epoch {epoch_in_tier}\")\n",
        "\n",
        "                # --- TRAIN ---\n",
        "                self.model_adapter.model.train()\n",
        "                epoch_losses = []\n",
        "                progress = tqdm.tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "                for batch in progress:\n",
        "                    metrics = self.model_adapter.train_step(batch)\n",
        "                    epoch_losses.append(metrics['loss'])\n",
        "                    progress.set_postfix({'loss': metrics['loss']})\n",
        "\n",
        "                # --- VALIDATE ---\n",
        "                print(\"Validating...\")\n",
        "                self.model_adapter.model.eval()\n",
        "                val_losses = []\n",
        "                val_accs = []\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for batch in tqdm.tqdm(val_loader, desc=\"Validation\"):\n",
        "                        out = self.model_adapter.validation_step(batch)\n",
        "                        acc = self.evaluation_service.compute_accuracy(out['logits'], out['labels'])\n",
        "\n",
        "                        val_losses.append(out['loss'])\n",
        "                        val_accs.append(acc)\n",
        "\n",
        "                avg_val_loss = sum(val_losses) / len(val_losses) if val_losses else 0.0\n",
        "                avg_val_acc = sum(val_accs) / len(val_accs) if val_accs else 0.0\n",
        "\n",
        "                print(f\"Validation: Loss={avg_val_loss:.4f}, Accuracy={avg_val_acc:.4f}\")\n",
        "\n",
        "                # Record metrics for Moving Average Calculation\n",
        "                self.evaluation_service.record_metrics(avg_val_loss, avg_val_acc)\n",
        "\n",
        "                # --- PATIENCE CHECK ---\n",
        "                if avg_val_acc > best_acc_in_tier:\n",
        "                    best_acc_in_tier = avg_val_acc\n",
        "                    patience_counter = 0  # Reset counter\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    print(f\"No improvement in accuracy. Patience: {patience_counter}/{patience_limit}\")\n",
        "\n",
        "                # --- CURRICULUM CHECK ---\n",
        "                should_advance = self.curriculum.should_advance(self.evaluation_service)\n",
        "\n",
        "                # --- CHECKPOINT ---\n",
        "                metrics_state = self.evaluation_service.get_latest_metrics()\n",
        "\n",
        "                # We save every epoch as 'latest', but only mark 'is_best' if we are advancing\n",
        "                self.checkpoint_manager.save(\n",
        "                    model_state=self.model_adapter.get_state_dict(),\n",
        "                    optimizer_state=self.model_adapter.get_optimizer_state_dict(),\n",
        "                    curriculum_state=self.curriculum.get_config_state(),\n",
        "                    metrics=metrics_state,\n",
        "                    tier=current_tier,\n",
        "                    is_best=should_advance\n",
        "                )\n",
        "\n",
        "                if should_advance:\n",
        "                    # Logic: If we qualify to advance, we do so immediately.\n",
        "                    self.curriculum.advance_tier()\n",
        "                    # Reset metric history so the moving average starts fresh for the new tier\n",
        "                    new_lr = self.curriculum.get_current_lr()\n",
        "                    for param_group in self.model_adapter.optimizer.param_groups:\n",
        "                        param_group['lr'] = new_lr\n",
        "                    self.evaluation_service.reset_history()\n",
        "                    # Break the inner epoch loop to restart outer loop with new Tier data\n",
        "                    break\n",
        "\n",
        "                if patience_counter >= patience_limit:\n",
        "                    print(f\"\\n[STOP] Patience exhausted at Tier {current_tier}. Model is not improving.\")\n",
        "                    print(\"Stopping training to prevent overfitting or wasted compute.\")\n",
        "                    return  # Exit the entire training function\n"
      ],
      "metadata": {
        "id": "LCXv3RBvAPD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# QUESTIONS_DIR = \"/content/drive/MyDrive/Colab Notebooks/FYP/dataset/clevr_kaggle/CLEVR_v1.0/questions\"\n",
        "# IMAGES_DIR    = \"/content/drive/MyDrive/Colab Notebooks/FYP/dataset/clevr_kaggle/CLEVR_v1.0/images\"\n",
        "\n",
        "QUESTIONS_DIR = \"/content/clevr_dataset/questions\"\n",
        "IMAGES_DIR    = \"/content/clevr_dataset/images\"\n",
        "\n",
        "def main():\n",
        "  # Mount Drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Competence-Aware Curriculum VQA Training\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "    parser.add_argument(\"--use_sspl\", action=\"store_true\")\n",
        "\n",
        "    # IMPORTANT for Colab\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    # 1Ô∏è‚É£ Build answer vocabulary from TRAIN questions\n",
        "    print(\"Building answer vocabulary...\")\n",
        "    tier_paths = [os.path.join(QUESTIONS_DIR, f\"CLEVR_train_questions_L{i}.json\") for i in [1,2,3,4,5]]\n",
        "    answer2id = build_answer_vocab(\n",
        "        tier_paths\n",
        "    )\n",
        "\n",
        "    print(f\"Answer vocab size: {len(answer2id)}\")\n",
        "\n",
        "    # 2Ô∏è‚É£ Initialize trainer\n",
        "    trainer = CurriculumTrainer(\n",
        "        questions_dir=QUESTIONS_DIR,\n",
        "        images_dir=IMAGES_DIR,\n",
        "        answer2id=answer2id,\n",
        "        batch_size=args.batch_size,\n",
        "        use_sspl=args.use_sspl\n",
        "    )\n",
        "\n",
        "    print(\"üöÄ Starting Curriculum Training...\")\n",
        "    trainer.train()\n"
      ],
      "metadata": {
        "id": "KljET_qzplC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "SqIKhEDKqamp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "i57YaF9p5KOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GIWtN6ic2q81"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}